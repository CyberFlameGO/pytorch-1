# Segmentation Models

Uses [Segmentation Models](https://github.com/qubvel/segmentation_models.pytorch) ([documentation](https://smp.readthedocs.io/en/v0.2.1/)). 

Uses PyTorch 1.9.0, CUDA 11.1 and Segmentation Models 0.2.1.

Though Segmentation Models is installed via a wheel file, you can find its source code \
inside the container in:

```bash
/opt/segmentation_models
```

Additional code is located in:

```bash
/opt/segmentation_models_ext
```

## Version

Segmentation Models github repo hash:

```
a288d337821716ad67125127b5dd96a1cd833391
```

and timestamp:

```
October 26, 2021
```

## Docker

### Quick start

* Log into registry using *public* credentials:

  ```bash
  docker login -u public -p public public.aml-repo.cms.waikato.ac.nz:443 
  ```

* Pull and run image (adjust volume mappings `-v`):

  ```bash
  docker run --runtime=nvidia --shm-size 8G --net=host \
    -v /local/dir:/container/dir \
    -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```

  **NB:** For docker versions 19.03 (`docker version`) and newer, use `--gpus=all` instead of `--runtime=nvidia`.

* If need be, remove all containers and images from your system:

  ```bash
  docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q) && docker system prune -a
  ```

### Docker hub

The image is also available from [Docker hub](https://hub.docker.com/u/waikatodatamining):

```
waikatodatamining/segmentation_models:0.2.1
```

### Build local image

* Build the image from Docker file (from within /path_to/segmentation_models/0.2.1)

  ```bash
  docker build -t segmentation_models .
  ```
  
* Run the container

  ```bash
  docker run --runtime=nvidia --shm-size 8G --net=host -v /local/dir:/container/dir -it segmentation_models
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container

### Pre-built images

* Build

  ```bash
  docker build -t segmentation_models:0.2.1 .
  ```
  
* Tag

  ```bash
  docker tag \
    segmentation_models:0.2.1 \
    public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```
  
* Push

  ```bash
  docker push public-push.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public-push.aml-repo.cms.waikato.ac.nz:443
  ```
  
* Pull

  If image is available in aml-repo and you just want to use it, you can pull using following command and then [run](#run).

  ```bash
  docker pull public.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public.aml-repo.cms.waikato.ac.nz:443
  ```
  Then tag by running:
  
  ```bash
  docker tag \
    public.aml-repo.cms.waikato.ac.nz:443/pytorch/segmentation_models:0.2.1 \
    pytorch/segmentation_models:0.2.1
  ```
  
* <a name="run">Run</a>

  ```bash
  docker run --runtime=nvidia --shm-size 8G --net=host \
    -v /local/dir:/container/dir -it pytorch/segmentation_models:0.2.1
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container


## Permissions

When running the docker container as regular use, you will want to set the correct
user and group on the files generated by the container (aka the user:group launching
the container):

```bash
docker run -u $(id -u):$(id -g) -e USER=$USER ...
```

## Caching

Segmentation models will download pretrained models and cache them locally. To avoid having
to download them constantly, you can the cache directory to the host machine:

* when running the container as `root`

  ```bash
  -v /some/where/cache:/root/.cache \
  ```

* when running the container as current user

  ```bash
  -v /some/where/cache:/.cache \
  ```


## Scripts

The following additional scripts are available:

* `sm_train` - for training models (calls `/opt/segmentation_models_ext/train.py`)
* `sm_predict` - for generating batch predictions on images (calls `/opt/segmentation_models_ext/predict.py`)
* `sm_predict_redis` - for generating batch predictions on images via redis backend (calls `/opt/segmentation_models_ext/predict_redis.py`)
* `sm_test_image_redis` - for uploading an image to the redis backend (calls `/opt/segmentation_models_ext/test_image_redis.py`)


## Config file
Config files can be in JSON (.json) or YAML (.yaml, .yml). 

Here is an example generated for the `cars_segmentation.py` example:

```json
{
    "model": {
        "class": "segmentation_models_pytorch.FPN",
        "parameters": {
            "encoder_name": "se_resnext50_32x4d",
            "encoder_weights": "imagenet",
            "activation": "sigmoid"
        }
    },
    "augmentation": {
        "train": {"__version__": "1.1.0", "transform": {"__class_fullname__": "Compose", "p": 1.0, "transforms": [{"__class_fullname__": "HorizontalFlip", "always_apply": false, "p": 0.5}, {"__class_fullname__": "ShiftScaleRotate", "always_apply": false, "p": 1, "shift_limit_x": [-0.1, 0.1], "shift_limit_y": [-0.1, 0.1], "scale_limit": [-0.5, 0.5], "rotate_limit": [0, 0], "interpolation": 1, "border_mode": 0, "value": null, "mask_value": null}, {"__class_fullname__": "PadIfNeeded", "always_apply": true, "p": 1.0, "min_height": 320, "min_width": 320, "pad_height_divisor": null, "pad_width_divisor": null, "border_mode": 0, "value": null, "mask_value": null}, {"__class_fullname__": "RandomCrop", "always_apply": true, "p": 1.0, "height": 320, "width": 320}, {"__class_fullname__": "IAAAdditiveGaussianNoise", "always_apply": false, "p": 0.2, "loc": 0, "scale": [2.5500000000000003, 12.75], "per_channel": false}, {"__class_fullname__": "IAAPerspective", "always_apply": false, "p": 0.5, "scale": [0.05, 0.1], "keep_size": true}, {"__class_fullname__": "OneOf", "p": 0.9, "transforms": [{"__class_fullname__": "CLAHE", "always_apply": false, "p": 1, "clip_limit": [1, 4.0], "tile_grid_size": [8, 8]}, {"__class_fullname__": "RandomBrightness", "always_apply": false, "p": 1, "limit": [-0.2, 0.2]}, {"__class_fullname__": "RandomGamma", "always_apply": false, "p": 1, "gamma_limit": [80, 120], "eps": null}]}, {"__class_fullname__": "OneOf", "p": 0.9, "transforms": [{"__class_fullname__": "IAASharpen", "always_apply": false, "p": 1, "alpha": [0.2, 0.5], "lightness": [0.5, 1.0]}, {"__class_fullname__": "Blur", "always_apply": false, "p": 1, "blur_limit": [3, 3]}, {"__class_fullname__": "MotionBlur", "always_apply": false, "p": 1, "blur_limit": [3, 3]}]}, {"__class_fullname__": "OneOf", "p": 0.9, "transforms": [{"__class_fullname__": "RandomContrast", "always_apply": false, "p": 1, "limit": [-0.2, 0.2]}, {"__class_fullname__": "HueSaturationValue", "always_apply": false, "p": 1, "hue_shift_limit": [-20, 20], "sat_shift_limit": [-30, 30], "val_shift_limit": [-20, 20]}]}], "bbox_params": null, "keypoint_params": null, "additional_targets": {}}},
        "test": {"__version__": "1.1.0", "transform": {"__class_fullname__": "Compose", "p": 1.0, "transforms": [{"__class_fullname__": "PadIfNeeded", "always_apply": false, "p": 1.0, "min_height": 384, "min_width": 480, "pad_height_divisor": null, "pad_width_divisor": null, "border_mode": 4, "value": null, "mask_value": null}], "bbox_params": null, "keypoint_params": null, "additional_targets": {}}}
    },
    "train": {
        "num_epochs": 10,
        "loss": {
            "class": "segmentation_models_pytorch.utils.losses.DiceLoss",
            "parameters": {}
        },
        "metrics": [{
            "class": "segmentation_models_pytorch.utils.metrics.IoU",
            "parameters": {
                "threshold": 0.5
            }
        }],
        "optimizer": {
            "class": "torch.optim.Adam",
            "parameters": {
                "lr": 0.0001
            }
        },
        "lr_schedule": {"25": 1e-5}
    },
    "classes": ["Sky", "Building", "Pole", "Road", "Pavement", "Tree", "SignSymbol", "Fence", "Car", "Pedestrian", "Bicyclist", "Unlabelled"],
    "classes_to_use": ["Car"]
}
```

## Augmentations

The following Python snippets show how to generate JSON output of albumentation augmentation
pipelines, which can be copy/pasted into the config file under the `train_aug`/`test_aug` keys. 

### Train

```python
import albumentations as albu

def get_augmentation():
    _transform = [
        albu.HorizontalFlip(p=0.5),
        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),
        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),
        albu.RandomCrop(height=320, width=320, always_apply=True),
        albu.IAAAdditiveGaussianNoise(p=0.2),
        albu.IAAPerspective(p=0.5),
        albu.OneOf(
            [
                albu.CLAHE(p=1),
                albu.RandomBrightness(p=1),
                albu.RandomGamma(p=1),
            ],
            p=0.9,
        ),
        albu.OneOf(
            [
                albu.IAASharpen(p=1),
                albu.Blur(blur_limit=3, p=1),
                albu.MotionBlur(blur_limit=3, p=1),
            ],
            p=0.9,
        ),
        albu.OneOf(
            [
                albu.RandomContrast(p=1),
                albu.HueSaturationValue(p=1),
            ],
            p=0.9,
        ),
    ]
    return albu.Compose(_transform)

if __name__ == "__main__":
    albu.save(get_augmentation(), "./train_aug.json", data_format='json')
```

### Test

```python
import albumentations as albu

def get_augmentation():
    test_transform = [
        albu.PadIfNeeded(384, 480)
    ]
    return albu.Compose(test_transform)

if __name__ == "__main__":
    albu.save(get_augmentation(), "./test_aug.json", data_format='json')
```


## Examples

### Car segmentation

A simple [car segmentation example](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb) 
is included in the image:

```
cd /opt/segmentation_models_ext
python3 cars_segmentation.py
```

**NB:** This will clone the repository with the [data](https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid)
in the `/opt/segmentation_models_ext` directory. If you want to avoid re-cloning it, then copy 
the `cars_segmentation.py` script into a directory that is mapped to a directory on the host, e.g.:

```bash
docker run --runtime=nvidia -u $(id -u):$(id -g) -e USER=$USER --shm-size 8G --net=host \
    -v `pwd`:/workspace \
    -v `pwd`/cache:/.cache \
    -it waikatodatamining/segmentation_models:0.2.1

cp /opt/segmentation_models_ext/cars_segmentation.py
cd /workspace/
python3 cars_segmentation.py
```
